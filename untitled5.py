# -*- coding: utf-8 -*-
"""Untitled5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ngTQzywbOewWZDlIzEGTyfeFwzr21Tgc
"""



"""Importing Libraries"""

import pandas as pd
import numpy as np

import os
import sys

# librosa is a Python library for analyzing audio and music. It can be used to extract the data from the audio files we will see it later.
import librosa
import librosa.display
import seaborn as sns
import matplotlib.pyplot as plt


from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.model_selection import train_test_split

# to play the audio files
from IPython.display import Audio

import keras
from keras.callbacks import ReduceLROnPlateau
from keras.models import Sequential
from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout, BatchNormalization
#from keras.utils import np_utils, to_categorical
from tensorflow.keras.utils import to_categorical

from keras.callbacks import ModelCheckpoint

import warnings
if not sys.warnoptions:
    warnings.simplefilter("ignore")
warnings.filterwarnings("ignore", category=DeprecationWarning)

from google.colab import drive
drive.mount('/content/drive')

"""Data Preparation"""

# Paths for data.

Ravdess = "/content/drive/MyDrive/dataset.dw/Ravdess/"
Crema = "/content/drive/MyDrive/dataset.dw/Crema/"
Tess = "/content/drive/MyDrive/dataset.dw/Tess/"
Savee = "/content/drive/MyDrive/dataset.dw/Savee/"

"""1. Ravdess Dataframe"""

import os
import pandas as pd

Ravdess = '/content/drive/MyDrive/dataset.dw/Ravdess'  # Replace this with your actual directory

file_emotion = []
file_path = []

for dirpath, dirnames, filenames in os.walk(Ravdess):
    for file in filenames:
        if file.endswith('.wav'):  # Assuming audio files are in WAV format
            part = file.split('-')
            if len(part) >= 2:  # Check if the split provides enough elements
                file_emotion.append(part[2])  # Modify this part to extract the correct emotion information
                file_path.append(os.path.join(dirpath, file))
            else:
                print(f"Issue with file: {file}. Not enough parts after splitting. Skipping this file.")

# Create the DataFrame with emotions and paths
Ravdess_df = pd.DataFrame({'Emotions': file_emotion, 'Path': file_path})

# Display the DataFrame
print(Ravdess_df.head())

import os
import pandas as pd

Ravdess = '/content/drive/MyDrive/dataset.dw/Ravdess'  # Replace this with your actual directory

file_emotion = []
file_path = []

# Mapping of numeric labels to emotions
emotion_map = {
    '01': 'neutral',
    '02': 'calm',
    '03': 'happy',
    '04': 'sad',
    '05': 'angry',
    '06': 'fear',
    '07': 'disgust',
    '08': 'surprise'
}

for dirpath, dirnames, filenames in os.walk(Ravdess):
    for file in filenames:
        if file.endswith('.wav'):  # Assuming audio files are in WAV format
            part = file.split('-')
            if len(part) >= 3:  # Check if the split provides enough elements for emotion extraction
                # Extract the emotion code from the filename
                emotion_code = part[2]
                # Map the emotion code to its corresponding label
                emotion_label = emotion_map.get(emotion_code, 'Unknown')
                file_emotion.append(emotion_label)
                file_path.append(os.path.join(dirpath, file))
            else:
                print(f"Issue with file: {file}. Not enough parts after splitting. Skipping this file.")

# Create the DataFrame with emotions and paths
Ravdess_df = pd.DataFrame({'Emotions': file_emotion, 'Path': file_path})

# Display the DataFrame
print(Ravdess_df.head())

"""2. Crema DataFrame"""

crema_directory_list = os.listdir(Crema)

file_emotion = []
file_path = []

for file in crema_directory_list:
    # storing file paths
    file_path.append(Crema + file)
    # storing file emotions
    part=file.split('_')
    if part[2] == 'SAD':
        file_emotion.append('sad')
    elif part[2] == 'ANG':
        file_emotion.append('angry')
    elif part[2] == 'DIS':
        file_emotion.append('disgust')
    elif part[2] == 'FEA':
        file_emotion.append('fear')
    elif part[2] == 'HAP':
        file_emotion.append('happy')
    elif part[2] == 'NEU':
        file_emotion.append('neutral')
    else:
        file_emotion.append('Unknown')

# dataframe for emotion of files
emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])

# dataframe for path of files.
path_df = pd.DataFrame(file_path, columns=['Path'])
Crema_df = pd.concat([emotion_df, path_df], axis=1)
Crema_df.head()

"""3. TESS dataset"""

tess_directory_list = os.listdir(Tess)

file_emotion = []
file_path = []

for dir in tess_directory_list:
    directories = os.listdir(Tess + dir)
    for file in directories:
        part = file.split('.')[0]
        part = part.split('_')[2]
        if part=='ps':
            file_emotion.append('surprise')
        else:
            file_emotion.append(part)
        file_path.append(Tess + dir + '/' + file)

# dataframe for emotion of files
emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])

# dataframe for path of files.
path_df = pd.DataFrame(file_path, columns=['Path'])
Tess_df = pd.concat([emotion_df, path_df], axis=1)
Tess_df.head()

"""4. CREMA-D dataset"""

savee_directory_list = os.listdir(Savee)

file_emotion = []
file_path = []

for file in savee_directory_list:
    file_path.append(Savee + file)
    part = file.split('_')[1]
    ele = part[:-6]
    if ele=='a':
        file_emotion.append('angry')
    elif ele=='d':
        file_emotion.append('disgust')
    elif ele=='f':
        file_emotion.append('fear')
    elif ele=='h':
        file_emotion.append('happy')
    elif ele=='n':
        file_emotion.append('neutral')
    elif ele=='sa':
        file_emotion.append('sad')
    else:
        file_emotion.append('surprise')

# dataframe for emotion of files
emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])

# dataframe for path of files.
path_df = pd.DataFrame(file_path, columns=['Path'])
Savee_df = pd.concat([emotion_df, path_df], axis=1)
Savee_df.head()

# creating Dataframe using all the 4 dataframes we created so far.
data_path = pd.concat([Ravdess_df, Crema_df, Tess_df, Savee_df], axis = 0)
data_path.to_csv("data_path.csv",index=False)
data_path.head()

"""Data Visualisation and Exploration"""

import matplotlib.pyplot as plt
import seaborn as sns

# Assuming your DataFrame is named 'Ravdess_df' and contains the 'Emotions' column
plt.figure(figsize=(8, 6))
plt.title('Count of Emotions', size=16)
sns.countplot(data=Ravdess_df, x='Emotions')  # Specify 'x' as 'Emotions' column from Ravdess_df
plt.ylabel('Count', size=12)
plt.xlabel('Emotions', size=12)
sns.despine(top=True, right=True, left=False, bottom=False)
plt.show()

def create_waveplot(data, sr, e):
    plt.figure(figsize=(10, 3))
    plt.title('Waveplot for audio with {} emotion'.format(e), size=15)
    librosa.display.waveplot(data, sr=sr)
    plt.show()

def create_spectrogram(data, sr, e):
    # stft function converts the data into short term fourier transform
    X = librosa.stft(data)
    Xdb = librosa.amplitude_to_db(abs(X))
    plt.figure(figsize=(12, 3))
    plt.title('Spectrogram for audio with {} emotion'.format(e), size=15)
    librosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='hz')
    #librosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='log')
    plt.colorbar()

import matplotlib.pyplot as plt
import librosa
import librosa.display
from IPython.display import Audio

def create_waveplot(data, sr, e):
    plt.figure(figsize=(10, 3))
    plt.title(f'Waveplot for audio with {e} emotion', size=15)
    plt.plot(data)
    plt.xlabel('Sample')
    plt.ylabel('Amplitude')
    plt.show()

def create_spectrogram(data, sr, e):
    plt.figure(figsize=(10, 4))
    spectrogram = librosa.feature.melspectrogram(y=data, sr=sr)
    librosa.display.specshow(librosa.power_to_db(spectrogram, ref=np.max), sr=sr, x_axis='time', y_axis='mel')
    plt.colorbar(format='%+2.0f dB')
    plt.title(f'Spectrogram for audio with {e} emotion', size=15)
    plt.show()

emotion = 'fear'
path = np.array(data_path.Path[data_path.Emotions == emotion])[1]
data, sampling_rate = librosa.load(path)

create_waveplot(data, sampling_rate, emotion)
create_spectrogram(data, sampling_rate, emotion)
Audio(path)

emotion='angry'
path = np.array(data_path.Path[data_path.Emotions==emotion])[1]
data, sampling_rate = librosa.load(path)
create_waveplot(data, sampling_rate, emotion)
create_spectrogram(data, sampling_rate, emotion)
Audio(path)

emotion='sad'
path = np.array(data_path.Path[data_path.Emotions==emotion])[1]
data, sampling_rate = librosa.load(path)
create_waveplot(data, sampling_rate, emotion)
create_spectrogram(data, sampling_rate, emotion)
Audio(path)

emotion='happy'
path = np.array(data_path.Path[data_path.Emotions==emotion])[1]
data, sampling_rate = librosa.load(path)
create_waveplot(data, sampling_rate, emotion)
create_spectrogram(data, sampling_rate, emotion)
Audio(path)

"""Data Augmentation"""

def noise(data):
    noise_amp = 0.035*np.random.uniform()*np.amax(data)
    data = data + noise_amp*np.random.normal(size=data.shape[0])
    return data

def stretch(data, rate=0.8):
    return librosa.effects.time_stretch(data, rate)

def shift(data):
    shift_range = int(np.random.uniform(low=-5, high = 5)*1000)
    return np.roll(data, shift_range)

def pitch(data, sampling_rate, pitch_factor=0.7):
    return librosa.effects.pitch_shift(data, sampling_rate, pitch_factor)

# taking any example and checking for techniques.
path = np.array(data_path.Path)[1]
data, sample_rate = librosa.load(path)

"""1. Simple Audio"""

import matplotlib.pyplot as plt
import librosa
from IPython.display import Audio

# Assuming data and sample_rate are defined

# Plot waveform using Matplotlib's plot function
plt.figure(figsize=(14, 4))
plt.plot(data)
plt.title("Waveform")
plt.xlabel("Sample")
plt.ylabel("Amplitude")
plt.show()

# Play audio
Audio(data, rate=sample_rate)

"""2. Noise Injection"""

x = noise(data)
plt.figure(figsize=(14,4))
#librosa.display.waveplot(y=x, sr=sample_rate)
# Replace this line using Matplotlib's plot function to plot the waveform
plt.figure(figsize=(14, 4))
plt.plot(x)
plt.title("Waveform")
plt.xlabel("Sample")
plt.ylabel("Amplitude")
plt.show()

Audio(x, rate=sample_rate)

"""3.Stretching"""

import matplotlib.pyplot as plt
import librosa
from IPython.display import Audio

def stretch(data, rate=0.8):
    return librosa.effects.time_stretch(data, rate=rate)

# Assuming data is your audio signal and sample_rate is your sampling rate

# Stretch the audio by a factor of 0.8 (80% of original duration)
x = stretch(data, rate=0.8)

# Plot the stretched waveform
plt.figure(figsize=(14, 4))
librosa.display.waveshow(x, sr=sample_rate)
plt.title("Stretched Waveform")
plt.xlabel("Time")
plt.ylabel("Amplitude")
plt.show()

# Play the stretched audio
Audio(x, rate=sample_rate)

"""4. Shifting"""

import matplotlib.pyplot as plt
import librosa
import numpy as np
from IPython.display import Audio

def shift(data, shift_amount=1000):
    return np.roll(data, shift_amount)

# Assuming data is your audio signal and sample_rate is your sampling rate

# Ensure data is a numpy array
data = np.array(data)

# Shift the audio by a certain amount (e.g., 1000 steps)
x = shift(data, shift_amount=1000)

# Plot the original waveform using Matplotlib
plt.figure(figsize=(14, 4))
plt.plot(data)
plt.title("Original Waveform")
plt.xlabel("Sample")
plt.ylabel("Amplitude")
plt.show()

# Plot the shifted waveform using Matplotlib
plt.figure(figsize=(14, 4))
plt.plot(x)
plt.title("Shifted Waveform")
plt.xlabel("Sample")
plt.ylabel("Amplitude")
plt.show()

# Play the shifted audio
Audio(x, rate=sample_rate)

"""5. Pitch"""

import matplotlib.pyplot as plt
import librosa
from IPython.display import Audio

def pitch(data, sampling_rate, pitch_factor=0.7):
    return librosa.effects.pitch_shift(data, sr=sampling_rate, n_steps=int(pitch_factor * 12))

# Assuming data is your audio signal and sample_rate is your sampling rate

# Perform pitch shifting with a pitch factor of 0.7 (you may adjust this)
x = pitch(data, sample_rate, pitch_factor=0.7)

# Plot the waveform of the shifted audio using Matplotlib
plt.figure(figsize=(14, 4))
librosa.display.waveshow(x, sr=sample_rate)
plt.title("Pitch-shifted Waveform")
plt.xlabel("Time")
plt.ylabel("Amplitude")
plt.show()

# Play the pitch-shifted audio
Audio(x, rate=sample_rate)

"""Feature Extraction"""

def extract_features(data):
    # ZCR
    result = np.array([])
    zcr = np.mean(librosa.feature.zero_crossing_rate(y=data).T, axis=0)
    result=np.hstack((result, zcr)) # stacking horizontally

    # Chroma_stft
    stft = np.abs(librosa.stft(data))
    chroma_stft = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T, axis=0)
    result = np.hstack((result, chroma_stft)) # stacking horizontally

    # MFCC
    mfcc = np.mean(librosa.feature.mfcc(y=data, sr=sample_rate).T, axis=0)
    result = np.hstack((result, mfcc)) # stacking horizontally

    # Root Mean Square Value
    rms = np.mean(librosa.feature.rms(y=data).T, axis=0)
    result = np.hstack((result, rms)) # stacking horizontally

    # MelSpectogram
    mel = np.mean(librosa.feature.melspectrogram(y=data, sr=sample_rate).T, axis=0)
    result = np.hstack((result, mel)) # stacking horizontally

    return result

def get_features(path):
    # duration and offset are used to take care of the no audio in start and the ending of each audio files as seen above.
    data, sample_rate = librosa.load(path, duration=2.5, offset=0.6)

    # without augmentation
    res1 = extract_features(data)
    result = np.array(res1)

    # data with noise
    noise_data = noise(data)
    res2 = extract_features(noise_data)
    result = np.vstack((result, res2)) # stacking vertically

    # data with stretching and pitching
    new_data = stretch(data)
    data_stretch_pitch = pitch(new_data, sample_rate)
    res3 = extract_features(data_stretch_pitch)
    result = np.vstack((result, res3)) # stacking vertically

    return result

print(data_path.Path)
print(data_path.Emotions)

X, Y = [], []

cut_data_path=data_path.Path[:500]
cut_data_emotion=data_path.Emotions[:500]
# Assuming data_path is an object with 'Path' and 'Emotions' attributes
for path, emotion in zip(cut_data_path, cut_data_emotion):
    feature = get_features(path)
    empo=[]
    if feature!=empo:  # Ensure features are obtained
        for ele in feature:
            X.append(ele)
            # Appending emotion 3 times as you mentioned 3 augmentation techniques per audio file
            Y.append(emotion)

len(X), len(Y), data_path.Path.shape

Features = pd.DataFrame(X)
Features['labels'] = Y
Features.to_csv('features.csv', index=False)
Features.head()

X = Features.iloc[: ,:-1].values
Y = Features['labels'].values

# As this is a multiclass classification problem onehotencoding our Y.
encoder = OneHotEncoder()
Y = encoder.fit_transform(np.array(Y).reshape(-1,1)).toarray()

# splitting data
x_train, x_test, y_train, y_test = train_test_split(X, Y, random_state=0, shuffle=True)
x_train.shape, y_train.shape, x_test.shape, y_test.shape

# scaling our data with sklearn's Standard scaler
scaler = StandardScaler()
x_train = scaler.fit_transform(x_train)
x_test = scaler.transform(x_test)
x_train.shape, y_train.shape, x_test.shape, y_test.shape

# making our data compatible to model.
x_train = np.expand_dims(x_train, axis=2)
x_test = np.expand_dims(x_test, axis=2)
x_train.shape, y_train.shape, x_test.shape, y_test.shape

model=Sequential()
model.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu', input_shape=(x_train.shape[1], 1)))
model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))

model.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu'))
model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))

model.add(Conv1D(128, kernel_size=5, strides=1, padding='same', activation='relu'))
model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))
model.add(Dropout(0.2))

model.add(Conv1D(64, kernel_size=5, strides=1, padding='same', activation='relu'))
model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))

model.add(Flatten())
model.add(Dense(units=32, activation='relu'))
model.add(Dropout(0.3))

model.add(Dense(units=8, activation='softmax'))
model.compile(optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = ['accuracy'])

model.summary()

rlrp = ReduceLROnPlateau(monitor='loss', factor=0.4, verbose=0, patience=2, min_lr=0.0000001)
history=model.fit(x_train, y_train, batch_size=64, epochs=50, validation_data=(x_test, y_test), callbacks=[rlrp])

print("Accuracy of our model on test data : " , model.evaluate(x_test,y_test)[1]*100 , "%")

epochs = [i for i in range(50)]
fig , ax = plt.subplots(1,2)
train_acc = history.history['accuracy']
train_loss = history.history['loss']
test_acc = history.history['val_accuracy']
test_loss = history.history['val_loss']

fig.set_size_inches(20,6)
ax[0].plot(epochs , train_loss , label = 'Training Loss')
ax[0].plot(epochs , test_loss , label = 'Testing Loss')
ax[0].set_title('Training & Testing Loss')
ax[0].legend()
ax[0].set_xlabel("Epochs")

ax[1].plot(epochs , train_acc , label = 'Training Accuracy')
ax[1].plot(epochs , test_acc , label = 'Testing Accuracy')
ax[1].set_title('Training & Testing Accuracy')
ax[1].legend()
ax[1].set_xlabel("Epochs")
plt.show()

# predicting on test data.
pred_test = model.predict(x_test)
y_pred = encoder.inverse_transform(pred_test)

y_test = encoder.inverse_transform(y_test)

df = pd.DataFrame(columns=['Predicted Labels', 'Actual Labels'])
df['Predicted Labels'] = y_pred.flatten()
df['Actual Labels'] = y_test.flatten()

df.head(10)

cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize = (12, 10))
cm = pd.DataFrame(cm , index = [i for i in encoder.categories_] , columns = [i for i in encoder.categories_])
sns.heatmap(cm, linecolor='white', cmap='Blues', linewidth=1, annot=True, fmt='')
plt.title('Confusion Matrix', size=20)
plt.xlabel('Predicted Labels', size=14)
plt.ylabel('Actual Labels', size=14)
plt.show()

print(classification_report(y_test, y_pred))